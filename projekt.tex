\chapter{Realizacja projektu}


\section{Wykorzystane technologie}

\subsection{Język programowania Python}

Kod projektu został napisany w języku Python. Wybór tego języka był spowodowany szeroką dostępnością bibliotek i
narzędzi wspomagających uczenie maszynowe. W szczególności należy zwrócić uwagę na biblioteki \textit{PyTorch} (użyta w projekcie) oraz \textit{Tensorflow} (alternatywa).
Są to dwie najpopularniejsze biblioteki do operowania na tensorach (tensor to uogólnienie macierzy na wiele wymiarów).
Obie pozwalają na przyśpieszanie obliczeń z użyciem zewnętrznych akceleratorów takich jak np. karty graficzne.

Podczas prac badawczych, wykorzystana wersja Pythona to \textit{3.12.0}.

\subsection{Biblioteka PyTorch}

Biblioteka PyTorch jest obecnie najpopularniejszą biblioteką uczenia maszynowego.
Pozwala ona projektować obliczenia w formie modułów i posiada silnik automatycznego różniczkowania grafu obliczeniowego (z ang. \textit{autograd}).
Owy silnik jest kluczowy z perspektywy trenowania sieci neuronowych, ponieważ jest w stanie optymalizować parametry modelu.
Warto zwrócić uwagę na to, że PyTorch stawia nacisk na przejrzystość obliczeń - dla porównania operacje w Tensorflow są nierzadko
ukryte pod gotowymi funkcjami (zob. listing \ref{lst:tensorflow_sample} i \ref{lst:pytorch_sample})

\begin{lstlisting}[language=ipython,caption={Przykładowa sieć neuronowa w Tensorflow},label={lst:tensorflow_sample}]
tensorflow_model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

tensorflow_model.compile() # Brak doglebnej kontroli nad przeplywem obliczen
\end{lstlisting}


\begin{lstlisting}[language=ipython,caption={Przykładowa sieć neuronowa w PyTorch},label={lst:pytorch_sample}]
class PyTorchNetwork(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x): # Dokladna kontrola nad przeplywem obliczen
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
\end{lstlisting}

PyTorch oferuje również zestaw narzędzi do przetwarzania danych, dostarcza również gotowe, pretrenowane modele dla najpopularniejszych architektur.
Przykładami są pakiety \textit{torchvision} lub \textit{torch.utils}. Cała biblioteka jest w stanie wykorzystywać karty graficzne (z ang. \textit{graphical processing unit, GPU}) do przyspieszania obliczeń.

\subsection{Biblioteka OpenCV}

Biblioteka OpenCV \cite{opencv} dostarcza implementacje wielu konwencjonalnych algorytmów wizji komputerowej.
OpenCV eksponuje interfejs programistyczny, dzięki któremu programista Pythona może komunikować się z implementacją algorytmów w C++.
To zapewnia szybkość wykonywania obliczeń, jednocześnie nie wymuszając na programiście pisania kodu w C++. Przykład wykorzystania biblioteki OpenCV widoczny jest na listingu \ref{lst:opencv_sample}
- kod wylicza momenty Hu.

OpenCV w niniejszej pracy jest wykorzystywane do automatycznej ekstrakcji kwadratów z komórkami na podstawie dużego zdjęcia spod mikroskopu.
Więcej informacji na ten temat w rozdziale \ref{sec:kwadraty}.

\begin{lstlisting}[language=ipython,caption={Obliczenie momentów Hu z użyciem OpenCV}, label={lst:opencv_sample}]
import cv2

image = cv2.imread('obraz.jpg', 0)  # 0 oznacza wczytanie obrazu w odcieniach szarosci

hu_moments = cv2.HuMoments(cv2.moments(image)).flatten()

print("Momenty Hu:")
for i in range(0, 7):
    print(f"Moment {i+1}: {hu_moments[i]}")
\end{lstlisting}

\subsection{Inne narzędzia}

Kod źródłowy projektu był przechowywany w repozytorium na platformie GitHub (dostępny pod adresem \url{https://github.com/matisiekpl/thesis}).
Wykresy były generowane za pomocą biblioteki \textit{matplotlib} \cite{matplotlib},
a metryki liczone z użyciem funkcji pakietu \textit{scikit-learn} \cite{scikit_learn}.

\subsection{Sprzęt}

Do prac badawczych został użyty Apple Macbook Pro 2020 (M1/16GB pamięci). Trenowanie sieci odbywało się na platformie \textit{kaggle.com}, która oferuje bezpłatne 30 godzin obliczeń co miesiąc z kartą graficzną \textit{NVIDIA P100}.


\section{Zbiór danych}

Wykorzystany w projekcie zbiór danych to kolekcja 170 tysięcy zdjęć komórek z rozmazów szpiku kostnego.
Zdjęcia mają nadane etykiety przez diagnostów i przedstawiają różne komórki widziane pod mikroskopem.
Próbki szpiku kostnego pochodzą z biopsji 945 pacjentów z Monachijskiego Labolatorium Białaczek (\textit{MLL Münchner Leukämielabor} \cite{mll}).
Akwizycja obrazu polegała na wykonaniu zdjęcia mikroskopią Brightfield'a z 40-krotnym powiększeniem.
Producentem mikroskopu było przedsiębiorstwo Fraunhofer IIS.

\begin{figure}
    \includegraphics[width=\textwidth]{images_count}
    \caption{Wizualizacja ilościowa rozkładu próbek w klasach w zbiorze danych}
    \label{fig:images_count_vis}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.9\textheight]{images_examples}
    \caption{Podgląd zdjęć komórek ze zbioru danych dla każdej z klas}
    \label{fig:images_examples}
\end{figure}

Komórki były barwione metodą Maya-Grünwalda-Giemsa \cite{histology}. Zbiór danych zawiera 21 klas.
Wizualizacja rozkładu klas jest widoczna na rys. \ref{fig:images_count_vis}.
Zbiór danych jest niezbalansowany, ponieważ ilość obrazów w każdej klasie mocno się różni.
Przykładowe zdjęcia są widoczne na rys \ref{fig:images_examples}.


\section{Architektury splotowych sieci neuronowych}

\subsection{\textit{EfficientNet B0}}

\subsection{\textit{EfficientNet B5}}

\subsection{\textit{ResNet}}


\section{Struktura projektu}

\subsection{Przygotowanie danych}

Zbiór danych to katalog ze zdjęciami. Zdjęcia są pogrupowane w podkatalogach, gdzie jego nazwa oznacza typ komórki.
Każdy podkatalog zawiera kolorowe zdjęcia w formacie \textit{.jpg} w rozmiarze \textit{250 pikseli x 250 pikseli}.
Przed treningiem sieci neuronowej konieczna jest przetwarzanie wstępne zbioru danych (z ang. \textit{preprocessing}).

\begin{lstlisting}[language=ipython,caption={Transformacja danych}, label={lst:transforms}]
transform = transforms.Compose([
    transforms.Resize((224, 224)), # przeskaluj obrazy
    transforms.RandomEqualize(1), # wyrownaj histogram z prawdopodobienstwem 1, czyli dla kazdego obrazu
    transforms.ToTensor(), # zamien na Tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[
        0.229, 0.224, 0.225]), # normalizuj wedlug mediany i odch. stand.
])
\end{lstlisting}

Do realizacji przygotowania danych wykorzystano pakiet \textit{torchvision.transforms}.
Kod transformacji jest przedstawiony na listingu \ref{lst:transforms}.
Wykonuje on przeskalowanie obrazu wejściowego do rozmiaru \textit{224 piksele na 224 piksele}.
Następnie wyrównuje histogram i normalizuje według średniej i odchylenia standardowego (rys. \ref{fig:transformations_example}).

Wyrównanie histogramu jest konieczne z powodu różnic w odcieniach barwienia Maya-Grünwalda-Giemsa.
Przy każdym wykonaniu rozmazu intensywność kolorów komórek jest inna.
Taka normalizacja zapobiega błędnemu nauczeniu się sieci neuronowej ze względu na odcień, a nie kształt i wygląd komórek.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{image_transform}
    \caption{Porównanie obrazu wejściowego przed i po transformacji}
    \label{fig:transformations_example}
\end{figure}

Po przetwarzaniu, zbiór danych został podzielony na zestaw treningowy i walidacyjny w proporcjach odpowiednio \textit{80:20}.
Zestaw treningowy służy do optymalizacji parametrów sieci neuronowej.
Zestaw walidacyjny natomiast jest używany do sprawdzania jakości modelu na obrazach, których sieć neuronowa "nigdy nie widziała".

\subsection{Trening sieci neuronowej}

Niniejsza praca ma za zadanie między innymi porównać różne architektury splotowych sieci neuronowych i ocenić,
która z nich wypada najlepiej w zadaniu klasyfikacji komórek z rozmazów szpiku kostnego.
Omówiony poniżej trening sieci neuronowej o architekturze \textit{EfficientNet B0} wygląda tak samo dla innych architektur, takich jak \textit{EfficientNet B5}, \textit{ResNet} i innych.
Tempo uczenia wynosiło \textit{0.001}, a ilość epok była równa 3.

Trenowanie sieci neuronowej z użyciem biblioteki PyTorch sprowadza się do iterowania po zbiorze wejściowym i wykonywania kroku optymalizacyjnego przy każdym przejściu.
Kod głównej funkcji treningowej jest widoczny na listingu \ref{lst:training}.

\begin{itemize}
    \item na listingu wykorzystano architekturę \textit{EfficientNet B0}
    \item jako funkcję straty użyto \textit{torch.nn.CrossEntropyLoss()}.
    Jest to funkcja używana do klasyfikatorów wieloklasowych.
    \item funkcja \textit{.to(device)} kopiuje tensory na urządzenie zewnętrzne, czyli kartę graficzną.
    Jest to wymagane, ponieważ model i dane muszą znajdować się w tym samym miejscu.
    Dzięki użyciu karty graficznej obliczenia są znacznie przyspieszone.
    \item \textit{optimizer.zero\_grad()} zeruje gradienty.
    Wywołanie tej funkcji jest konieczne z powodu domyślnego zachowania PyTorch, które akumuluje gradienty.
    Chcąc tego uniknąć, kod resetuje gradienty w grafie obliczeniowym przy każdej iteracji danych treningowych.
    \item \textit{loss.backward()} liczy gradienty w grafie obliczeniowym, zaś \textit{optimizer.step()} wykonuje krok optymalizacyjny.
\end{itemize}
\lstinputlisting[language=ipython,caption={Kod odpowiadający za trenowanie modelu \textit{EfficientNet B0}}, label={lst:training}]{train.py}
\lstinputlisting[language=ipython,caption={Kod odpowiadający ewaluację metryk}, label={lst:metrics}]{metrics.py}

\subsection{Ewaluacja jakości modelu}

Do sprawdzenia jakości modelu uczenia maszynowego konieczny jest dobór właściwych metryk.
Ze względu na to, że trenowany jest klasyfikator wieloklasowy, a zbiór danych nie jest zbalansowany, zwykłe metryki takie jak dokładność (z ang. \textit{accuracy}) nie wystarczą.
Dla każdej klasy jest liczona precyzja (z ang. \textit{precision}) i czułość (z ang. \textit{recall}).
Obie te metryki składają się do obliczenia wyniku F1 (z ang. \textit{F1-score}) \cite{geron}.
Co ważne, wynik F1 jest liczony z uwzględnieniem wag poszczególnych klas.
Z kolei funkcja \textit{confusion\_matrix} oblicza macierz pomyłek (czasem zwaną macierzą konfuzji).
Informuje ona o tym, jakie klasy są najczęściej mylone z jakimi.
Metryki są zapisywane do pliku tekstowego za pomocą funkcji \textit{log} oraz do plików obrazów za pomocą biblioteki \textit{matplotlib}.
Kod odpowiadając za liczenie metryk znajduje się na listingu \ref{lst:metrics}.


\section{Interfejs użytkownika}

W celu łatwego korzystania z modelu uczenia maszynowego przez diagnostów dostępna jest aplikacja internetowa do interakcji z nim.
Zrzut ekranu interfejsu użytkownika jest widoczny na rys. \ref{fig:ui}.
Została ona napisana z użyciem \textit{Vue} \cite{vue} - biblioteki do tworzenia interaktywnych interfejsów użytkownika dla sieci web.
Aplikacja łączy się z serwerem HTTP napisanym z użyciem biblioteki Flask \cite{flask}.
Ów serwer eksponuje interfejs REST API, który dla wysłanego zdjęcia zwraca rozkład prawdopodobieństwa (kod na listingu \ref{lst:serve}).
Przed wywołaniem modelu, funkcja upewnia się, że przychodzący obrazek jest poprawnego formatu.
Dopiero po zwalidowaniu poprawności danych, kod przystępuje do utworzenia rozkładu prawdopodobieństwa (zmienna \textit{result}).

\lstinputlisting[language=ipython,caption={Funkcja języka Python, która obsługuje ścieżkę HTTP POST}, label={lst:serve}]{serve.py}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{app}
    \caption{Aplikacja internetowa do rozpoznawania komórek}
    \label{fig:ui}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{arch}
    \caption{Architektura klient-serwer aplikacji internetowej i serwera Flask}
    \label{fig:arch}
\end{figure}

By skorzystać z aplikacji internetowej, należy przeciągnąć plik ze zdjęciem komórki do sekcji "Upuść tutaj rozmaz w PNG lub JPEG".
Po wysłaniu obrazu, użytkownik może wybrać model i kliknąć przycisk "Rozpoznaj rodzaj komórki".
Po chwili ukaże się tabela z rozkładem prawdopodobieństwa (po prawej stronie okna).
Stylizowanie aplikacji zostało wykonane z użyciem biblioteki CSS o nazwie \textit{Tailwind.css} \cite{tailwind}.
Komunikacja klient-serwer jest zapewniona przez bibliotekę \textit{Axios} \cite{axios}.
Architekturę przedstawia rys. \ref{fig:arch}.


\section{Algorytm ekstrakcji kwadratów komórek z dużego zdjęcia rozmazu}\label{sec:kwadraty}

Zbiór danych użyty w trakcie trenowania splotowej sieci neuronowej przedstawia komórki, które stanowią większość obrazka.
Innymi słowy, każdy obrazek stanowi wycinek dużego skanu z rozmazu.
Często jednak istnieje potrzeba zautomatyzowania procesu wycinania kwadratów przedstawiających komórki z dużego skanu.
Zaproponowano system korzystający z konwencjonalnych algorytmów wizji komputerowej do osiągnięcia tego zadania.

Niniejszy algorytm przyjmuje ścieżkę do obrazu przedstawiającego kilkanaście komórek różnego rodzaju i eksportuje
każdą komórkę do kwadratowego zdjęcia w oddzielnym katalogu wyjściowym.
Kod przedstawiony jest na listingu \ref{lst:extract}.

\lstinputlisting[language=ipython,caption={Algorytm ekstrakcji komórek z dużego skanu mikroskopowego}, label={lst:extract}]{extract.py}

Algorytm wykonuje następujące kroki w celu wyznaczenia komórek z obrazka:

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Im060_1}
    \caption{Wejście algorytmu}
    \label{fig:extract_input}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Im060_1_thresh}
    \caption{Obrazek po progowaniu}
    \label{fig:extract_thresh}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Im060_1_contours}
    \caption{Oznaczone kontury wraz z centroidami}
    \label{fig:extract_contours}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{cells}
    \caption{Wyekstraktowane kwadratowe obrazki}
    \label{fig:extract_squares}
\end{figure}

\begin{enumerate}
    \item Konwersja oryginalnego obrazu (rys. \ref{fig:extract_input}) na skalę szarości.
    Jest to wymagane, ponieważ progowanie w kroku drugim nie może odbywać się na obrazie kolorowym.
    Do konwersji zastosowano funkcję \textit{cv2.cvtColor} - pozwala ona konwertować obraz z jednej przestrzeni kolorów do innej.
    \item Progowanie z pomocą algorytmu Otsu.
    Jest to algorytm adaptacyjnynego progowania, często stosowany do analizy obrazów medycznych.
    Progowanie ma na celu wyodrębnienie ciemniejszych obszarów obrazu - cytoplazmy i jądra komórek (rys. \ref{fig:extract_thresh}). Otsu jest algorytmem progowania globalnego i opiera się na analizie histogramu.
    Dąży on do maksymalizacji wariancji międzyklasowej.
    \item Wyznaczenie konturów z pomocą funkcji \textit{cv2.findContours}. Kontur jest definiowany jako pewna ciągła krzywa, która łączy punkty o tej samej intensywności.
    W opisanym algorytmie, kontury informować o granicach komórki. Argumentem funkcji jest flaga \textit{cv2.RETR\_EXTERNAL}, która powoduje, że zwrócone zostają jedynie najbardziej zewnętrzne kontury - kontury wewnętrzne są ignorowane.
    \item Obliczanie centroidów konturów za pomocą momentów Hu. Każdy wyznaczony centroid jest traktowany jako środek komórki (rys. \ref{fig:extract_contours}). Współrzedne centroidów można wyznaczyć na podstawie momentów Hu, korzystając ze wzorów \ref{eq:hu_x} i \ref{eq:hu_y}.
    \begin{equation}
        cX = \dfrac{M_{10}}{M_{00}}\label{eq:hu_x}
    \end{equation}
    \begin{equation}
        cY = \dfrac{M_{01}}{M_{00}}\label{eq:hu_y}
    \end{equation}
    \item Wycięcie obszaru komórki za pomocą indeksów Python (rys. \ref{fig:extract_squares}). Obraz każdej znalezionej komórki trafia do osobnego pliku w folderze wyjściowym. Do zapisu na dysk wykorzystano funkcję \textit{matplotlib.pyplot.imsave()}.
\end{enumerate}
