\chapter{Zagadnienia teoretyczne}


\section{Sieci neuronowe}

Sieci neuronowe stanowią jedną z najpopularniejszych technik uczenia maszynowego.
Są one inspirowane działaniem ludzkiego mózgu.
Sieci składają się z neuronowów, pogrupowanych w połączone ze sobą warstwy.

Ogólna architektura sieci neuronowej:
\begin{itemize}
    \item Warstwa wejściowa - ta warstwa przyjmuje dane początkowe w formie tensora liczb rzeczywistych.
    Danymi wejściowymi mogą być poziomy jasności obrazu, dane o cenach lub sygnale (w przypadku prognozowania serii czasowych) lub osadzenia (z ang. \textit{embeddings}) dla modeli językowych.
    \item Warstwy ukryte - te warstwy przetwarzają dane wejściowe przez sekwencję transformacji matematycznych.
    Neurony są połączone z poprzednią warstwą, a siła ich połączenia jest wyrażana przez tak zwane wagi.
    Wartość, którą przyjmie dany neuron, jest liczona na podstawie ważonej sumy wartości neuronów z poprzedniej warstwy.
    Po obliczeniu sumy, stosowana jest funkcja aktywacji.
    Ma ona za zadanie wprowadzić nieliniowość obliczeń, która jest wymagana do rozpoznawania zaawansowanych wzorców.
    \item Warstwa wyjściowa - warstwa, która generuje końcowy tensor.
    Tensor może zawierać informacje, takie jak rozkład prawdopodobieństwa przyporządkowania do poszczególnych klas.
    Może również zawierać prognozowaną liczbę, w przypadku modeli regresyjnych.
\end{itemize}

Proces trenowania sieci neuronowej polega na modyfikowaniu wag pomiędzy połączeniami poszczególnych neuronów tak, by minimalizować różnicę między tensorem wyjściowym, a tensorem oczekiwanym.
Najczęściej stosuje się do tego zadania algorytmy optymalizacyjne oparte na pochodnych i gradientach.
Przykładem może być \textit{algorytm spadku wzdłuż gradientu}.
W rzeczywistych zadaniach stosuje się jednak bardziej wyszukane algorytmy takie jak algorytm stochastycznego spadku wzdłuż gradientu albo algorytm \textit{Adam}.

Sieci neuronowe okazały się dużym sukcesem w wielu trudnych zadaniach uczenia maszynowego.
Są z powodzeniem stosowane w takich branżach jak medycyna, rozrywka, cyberbezpieczeństwo czy motoryzacja.
Na przykład, zespół Tesla Vision trenuje duże sieci neuronowe by stworzyć pojazd autonomiczny, zdolny poruszać się bezpiecznie bez kierowcy.
Sieci neuronowe coraz częściej będą wykonywać określone zadania szybciej i skuteczniej niż człowiek.
Czas reakcji systemu autopilot samochodu autonomicznego jest znacznie krótszy niż czas reakcji kierowcy, co w dłuższej perspektywie może przyczynić się do zwiększenia bezpieczeństwa na drogach i szlakach komunikacyjnych.
Innym bardzo ważnym zastosowaniem sieci neuronowych jest medycyna.
Naukowcy z Google DeepMind stworzyli model \textit{AlphaFold}, który jest w stanie przewidywać molekularną strukturę białek.
Postęp informatyki w naukach biologicznych i chemicznych otwiera perspektywę na leczenie chorób, które dotychczas były nieuleczalne i umożliwia lepsze zrozumienie organizmu człowieka.


\section{Algorytm propagacji wstecznej}

Algorytm propagacji wstecznej to podstawowy algorytm używany do trenowania sieci neuronowych~\cite{geron}.
Trenowanie sieci neuronowej to nic innego jak aktualizacja wag neuronów w taki sposób, by minimalizować tak zwany błąd (z ang. \textit{loss}) w predykcjach.

Zasada działania algorytmu:
\begin{itemize}
    \item Inicjalizacja - wagi neuronów są ustawiane na losowe wartości.
    Często wartości są losowane z rozkładu prawdopodobieństwa Glorota.
    Użycie takiego rozkładu pozwala uodpornić proces treningu na problem zanikających i eksplodujących gradientów.
    \item Przejście w przód - dane wejściowe są przekazywane do sieci neuronowej i liczone są wartości neuronów w kolejnych warstwach.
    Ostatnia warstwa stanowi wyjście modelu.
    \item Obliczenie błędu (inaczej straty) - wyliczone wyjście modelu jest porównywane z oczekiwanym wyjściem (pochodzi ono z danych wejściowych w przypadku uczenia nadzorowanego).
    Używając różnych funkcji matematycznych, algorytm oblicza błąd, który informuje, jak bardzo predykcje odbiegają od docelowego wyjścia modelu.
    Najpopularniejsze funkcje błędu to błąd średniokwadratowy (z ang. \textit{Mean-Square Error} albo \textit{MSE}) bądź entropia krzyżowa (z ang. \textit{Cross Entropy}).
    Dla zadań klasyfikacji wieloklasowej często stosuje się entropię krzyżową, zaś dla regresji liniowej (np.
    przewidywanie ceny nieruchomości) można zastosować błąd średniokwadratowy.

    Błąd średniokwadratowy dany jest wzorem~\ref{eq:mse}, gdzie $n$ to liczba obserwacji, $y_i$ to rzeczywista wartość dla $i$-tej obserwacji $\hat{y}_i$ to przewidywana wartość dla $i$-tej obserwacji.
    \begin{equation}
        \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\label{eq:mse}
    \end{equation}

    Z kolei funkcja błędu entropii krzyżowej dana jest wzorem~\ref{eq:cross_entropy}, gdzie $N$ to liczba wszystkich próbek, a $K$ to liczba klas.
    Zmienne $y_{i,k}$ to prawdziwa etykieta dla próbki $i$ i klasy $k$, a $\hat{y}_{i,k}$ to przewidywane prawdopodobieństwo, że próbka $i$ należy do klasy $k$.
    \begin{equation}
        \mathcal{L}_{CE} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})\label{eq:cross_entropy}
    \end{equation}

    \item Propagacja błędu - błąd jest przekazywany wstecz od warstwy wyjściowej, przez warstwy ukryte aż do warstwy wejściowej.
    Przy przekazywaniu, liczone są pochodne cząstkowe funkcji błędu względem każdej z wag.
    \item Aktualizacja wag - wagi neuronów są aktualizowane za pomocą różnych algorytmów optymalizacyjnych.
    Jednym z najprostszych algorytmów jest metoda najszybszego spadku wzdłuż gradientu, która sprowadza się do wyliczenia następnej wartości za pomocą równania~\ref{eq:fastest_gradient_descent}.
    Warto zwrócić uwagę na hiperparametr $\alpha_k$ - jest to współczynnik uczenia.
    Informuje on algorytm o tempie optymalizacji.
    Gdy współczynnik będzie zbyt niski, proces trenowania będzie trwał bardzo długo i może dojść do sytuacji, gdzie model nigdy nie zdąży się wytrenować.
    Z kolei zbyt wysoka wartość współczynnika uczenia może spowodować trudności ze znalezieniem minimum globalnego i brak poprawy jakości modelu.
    \begin{equation}
        x_{k+1} = x_k - \alpha_k \nabla f(x_k)\label{eq:fastest_gradient_descent}
    \end{equation}
    \item Iteracja - kroki (oprócz inicjalizacji) są powtarzane tak długo, aż sieć neuronowa będzie dawała jakościowe predykcje.
\end{itemize}

Zagadnienie propagacji wstecznej jest bardzo szerokie - pomimo niezbyt skomplikowanego algorytmu, w trakcie trenowania sieci mogą pojawiać się problemy z tak zwanymi zanikającymi bądź eksplodującymi gradientami.
Wartość współczynnika uczenia nie musi być z góry ustalona, lecz może być wyliczana na bieżąco.
Przykładowo, wraz z czasem treningu współczynnik może być zmniejszany (nazwa metody w języku angielskim to \textit{learning rate decay}).
Zatem propagacja wsteczna jest bardzo przydatna w zagadnieniu trenowania sieci neuronowych i uczeniu głębokim, pomimo korzystania z prostych pojęć analizy matematycznej takich jak pochodna cząstkowa.


\section{Metryki}

Do stwierdzenia, który model uczenia maszynowego jest najlepszej jakości, wymagane są pewne metryki.
Dobór tych metryk zależy od zadania, jak i zbioru danych.
Jedną z podstawowych metryk jest dokładność (z ang. \textit{accuracy}) - równanie~\ref{eq:accuracy}.

\begin{equation}
    \text{Dokładność} = \frac{TP + TN}{TP + TN + FP + FN}\label{eq:accuracy}
\end{equation}

Czasem jednak metryka taka jak dokładność jest niewystarczająca.
W przypadku niezrównoważonych zbiorów danych najczęściej korzysta się z zestawu trzech metryk - precyzji, czułości i miary F1.

Precyzja jest definiowana jako:

\begin{equation}
    \text{Precyzja} = \frac{TP}{TP + FP}\label{eq:precision}
\end{equation}

Z kolei czułość dana jest wzorem:

\begin{equation}
    \text{Czułość} = \frac{TP}{TP + FN}\label{eq:recall}
\end{equation}

Metryką obejmującą precyzję i czułość predykcji dla danej klasy jest miara F1.
Jest to ich średnia harmoniczna (zob.
równanie ~\ref{eq:f1}).

\begin{equation}
    F1 = 2 \cdot \frac{\text{Precyzja} \cdot \text{Czułość}}{\text{Precyzja} + \text{Czułość}}\label{eq:f1}
\end{equation}

W omawianych wzorach \textit{TP}, \textit{FP}, \textit{TN}, \textit{FN} to ilość próbek prawidłowo lub nieprawidłowo zakwalifikowanych jako przypadki pozytywne lub negatywne (zob. tabelę ~\ref{tab:cases}).

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        & \textbf{Przewidywane Pozytywne} & \textbf{Przewidywane Negatywne} \\ \hline
        \textbf{Rzeczywiste Pozytywne} & TP                              & FN                              \\ \hline
        \textbf{Rzeczywiste Negatywne} & FP                              & TN                              \\ \hline
    \end{tabular}
    \caption{Macierz pomyłek}\label{tab:cases}
\end{table}


\section{Splotowe sieci neuronowe}

Splotowe sieci neuronowe to rodzaj sieci neuronowych dedykowanych do rozpoznawania przede wszystkim cech obrazów~\cite{geron}.
Znajdują one zastosowanie w zadaniach takich jak klasyfikacja obrazów, rozpoznawanie obiektów lub segmentacja.
Czasem są one używane do analizy wideo lub analizy sygnału w czasie.

Struktura splotowych sieci neuronowych sprowadza się do:
\begin{itemize}
    \item Warstwy wejściowej - ta warstwa przyjmuje zazwyczaj obraz jako tensor jasności poszczególnych pikseli.
    Obraz wejściowy może być w skali szarości (pojedynczy kanał) lub kolorowy - najczęściej składowe RGB pojedynczego piksela są kodowane przez 3 neurony.
    \item Warstw splotowych - te warstwy wykorzystują matematyczną operację splotu.
    Warstwy te zawierają macierze, zwane filtrami o ustalonym rozmiarze (najczęściej \textit{3x3} lub \textit{5x5}).
    Macierze są przesuwane od lewej do prawej strony, z góry do dołu po obrazie.
    Odpowiednie wartości filtrów są w stanie wzmocnić lub osłabić pewne cechy obrazu takie jak ostre krawędzie bądź rogi.
    Wartości macierzy są dobierane w procesie trenowania modelu.
    \item Warstw łączących - warstwy te mają za zadanie zmniejszyć wymiarowość danych z poprzedniej warstwy.
    Popularną funkcją łączącą jest uśrednianie - dla zadanego otoczenia pikseli (najczęściej \textit{2x2} lub \textit{3x3}) wybierana jest średnia wartość.
    Inną funkcją łączącą jest maksimum - z otoczenia wybierana jest wartość maksymalna.
    \item Warstw gęstych - po kilku warstwach splotowych, zaawansowane rozumowanie jest przeprowadzane przez warstwy gęste, czyli neurony połączone są w sposób “każdy z każdym“.
\end{itemize}

Główne zastosowanie splotowych sieci neuronowych to wykrywanie przedmiotów i ludzi na obrazach i nagraniach wideo,
opisywanie i wykrywanie anomalii w skanach medycznych oraz analiza sytuacji na drodze dla pojazdów autonomicznych.


\section{Problem klasyfikacji komórek krwi w rozmazach szpiku kostnego}

Problem opisu komórek w rozmazach krwi i szpiku kostnego jest bardzo ważny z perspektywy diagnostyki wielu chorób takich jak np.
nowotwory.
Po wykonaniu mikroskopowej fotografii rozmazu, lekarz diagnosta musi policzyć ilość komórek każdego typu w próbce.
Ilość komórek danego rodzaju może być przesłanką do zdiagnozowania nieprawidłowości w organiźmie.
Przykładowo, do stwierdzenia białaczki lekarz zlicza ilość limfoblastów w próbce w stosunku do wszystkich komórek.
Gdy ten stosunek przekracza pewien próg, jest to przesłanka mogąca świadczyć o obecności nowotworu.
Limfoblasty to forma niedojrzała limfocytów.
Ich zwiększone występowanie sugeruje nieprawidłowości w procesie rozwoju tych komórek do docelowej postaci.

Rodzaj komórki można stwierdzić przede wszystkim na podstawie jej wyglądu po barwieniu.
Zwraca się uwagę na wielkość jądra w stosunku do cytoplazmy, występowanie jąderek oraz ziarnistości~\cite{histology}.

Elementy morfotyczne krwi przedstawia rys. \ref{fig:electron_microscope}.
Górna część rysunku przedstawia obraz z mikroskopu optycznego, zaś dolna - z mikroskopu elektronowego, gdzie widocznych jest więcej szczegółów.
Objaśnienie: A - erytrocyt (widok z góry i widok przekroju), B - neutrofil, C - eozynofil, D - bazofil, E - limfocyt, F - monocyt. “za“ - ziarna azurochłonne, “zs“
- ziarna swoiste.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{morfotyczne}
    \caption{Elementy morfotyczne krwi w obrazie mikroskopu optycznego i elektronowego. Rysunek pochodzi z “Kompendium histologii“ autorstwa Tadeusza Cichockiego, Jana Litwina i Jadwigi Mireckiej \cite{histology}.}
    \label{fig:electron_microscope}
\end{figure}

Rozwój metod komputerowej klasyfikacji komórek jest bardzo istotny, ponieważ potencjalne zautomatyzowanie procesu zliczania komórek pozwoliłoby na szybsze stawianie diagnozy i zaoszczędzenie wielu godzin pracy diagnostów.

Obecne rozwiązania służące do opisu komórek w krwi lub szpiku kostnym bazują na cytometrii przepływowej (czyli analizie kąta odbicia fali światła).
Warto natomiast zwrócić uwagę na to, że zdjęcie komórki (to znaczy obrazowanie w zakresie światła widzialnego) jest wystarczające do rozpoznania typu komórki przez człowieka, a więc niesie dostateczną ilość informacji.
To oznacza, że odpowiednio zaawansowany model sztucznej inteligencji byłby w stanie klasyfikować komórki jedynie na podstawie wizji komputerowej, bez potrzeby analizy widma falowego lub kątów odbicia światła.


\section{Algorytmy widzenia komputerowego}

W niniejszym projekcie, oprócz widzenia komputerowego realizowanego przez splotowe sieci neuronowe, wykorzystano również inne algorytmy do analizy obrazów.
Jednym z nich jest mechanizm \textbf{progowania obrazu} (z ang. \textit{thresholding}).
Operacja ta przyporządkowuje wartość maksymalną piksela na obrazie wyjściowym, gdy poziom jasności na obrazie wejściowym jest większy od zadanego progu $k$.
Analogicznie, wyjściowy piksel przyjmuje wartość minimalną (najczęściej $0$), gdy piksel wejściowy jest poniżej progu $k$ lub wartość jest równa (zob.
równanie~\ref{eq:thresholding}).

\begin{equation}
    T(x, y) =
    \begin{cases}
        0 & \text{jeśli } I(x, y) < k \\
        1 & \text{jeśli } I(x, y) \geq k
    \end{cases}\label{eq:thresholding}
\end{equation}

Inną ważną metodą widzenia komputerowego są \textbf{momenty Hu} (zwane też momentami obrazów).
Momenty Hu to zestaw metryk służących do opis kształtu i właściwości pewnego regionu obrazu~\cite{vision}.
Korzystają one z analizy rozkładu jasności pikseli w obrazie.
W algorytmie ekstrakcji obrazów komórek z dużego zdjęcia rozmazu wykorzystywane są momenty centralne do wyznaczania punktów centralnych komórek.
Wzór na tak zwany moment centralny $\mu_{ij}$ przedstawia równanie~\ref{eq:central_moment}, zaś współrzędne punktu centralnego można wyznaczyć korzystając z równań~\ref{eq:centroid}.

\begin{equation}
    \mu_{ij} = \sum_x \sum_y x^i y^j I(x, y)\label{eq:central_moment}
\end{equation}

\begin{equation}
    \bar{x} = \frac{M_{10}}{M_{00}}, \quad \bar{y} = \frac{M_{01}}{M_{00}}\label{eq:centroid}
\end{equation}


\section{GradCAM - wytłumaczalne uczenie maszynowe}

GradCAM (z ang. \textit{Gradient-weighted Class Activation Mapping}) to technika, która umożliwia użytkownikom splotowych sieci neuronowych lepsze zrozumienie procesu podejmowania decyzji przez model uczenia maszynowego.
Jest to jedna z podstawowych metod zagadnienia wytłumaczalnego SI (z ang. \texit{explainable artificial intelligence}).
Po wykonaniu przejścia w przód sieci neuronowej GradCAM jest w stanie wygenerować mapę cieplną wskazującą obszary obrazu, które były istotne do podjęcia decyzji przez model.
Zasada działania algorytmu GradCAM:
\begin{itemize}
    \item Przejście w przód i propagacja wsteczna.
    Ten krok ma za zadanie obliczyć gradient $\pd{{y^c}}{{A^k}}$, gdzie $A^k$ jest $k$-tą mapą cech w ostatniej warstwie splotowej.
    \item Uśrednianie gradientów po wymiarach mapy cech, w celu uzyskania wag $a^c_k$ dla każdej mapy cech $k$ (zob.
    równanie~\ref{eq:gradcam_averaging}).
    \begin{equation}
        a^c_k = \frac{1}{Z} \sum_i \sum_j \pd{{y^c}}{{A^k_{ij}}}\label{eq:gradcam_averaging}
    \end{equation}
    \item Obliczenie ważonej mapy cech.
    Ostateczna mapa cieplna jest wyliczana jako kombinacja liniowa mapy aktywacji i wag $a^c_k$ (zob.
    równania~\ref{eq:gradcam_final} i~\ref{eq:relu}).
    \begin{equation}
        L^c_{\text{gradcam}} = \text{ReLU} \left( \sum_k \alpha^c_k A^k \right)\label{eq:gradcam_final}
    \end{equation}
    \begin{equation}
        \text{ReLU}(x) = \max(0, x).\label{eq:relu}
    \end{equation}
    \item Interpolacja do rozmiaru obrazu wejściowego-obliczona mapa cech $L^c_{\text{gradcam}}$ jest skalowana, by można było nałożyć ją na obraz wejściowy.
\end{itemize}

Skorzystanie z metod wytłumaczalnego uczenia maszynowego pozwala na zwiększenie transparentności modelu i lepsze zrozumienie procesu podejmowania decyzji.
Jest to szczególne ważne z punktu widzenia zastosowań sieci neuronowych w medycynie, ponieważ lekarz może dogłębniej przyjrzeć się przesłankom, co stoi za daną odpowiedzią modelu.
W przypadku zadania rozpoznawania rodzajów komórek z krwi obwodowej bądź szpiku mapa cieplna GradCAM wskazuje na te obszary komórki, po których można rozpoznać jej typ.
Innym przykładem zastosowania wytłumaczalnego uczenia maszynowego jest rozpoznawanie zmian nowotworowych na skanach mózgu z rezonansu magnetycznego.
Algorytm GradCAM może wskazać obszar na skanie, po którym model stwierdził obecność nowotworu~\cite{gradcam_brain_tumor}.